# ai_learning_game - Un jeu Ã©ducatif sur l'apprentissage automatique
# Copyright (C) 2025  AI Learning Game Contributors
#
# Ce programme est distribuÃ© sous licence Ã©ducative libre et non commerciale
# Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International
# avec des dispositions supplÃ©mentaires pour l'usage pÃ©dagogique.
#
# Vous Ãªtes libre de partager, adapter et enseigner avec ce matÃ©riel
# pour des usages non commerciaux en respectant l'attribution et le partage
# dans les mÃªmes conditions.
#
# Voir LICENSE_EDUCATIONAL pour les dÃ©tails complets de la licence.
# Source : https://github.com/[votre-repo]/ai_learning_game

try:
    import streamlit as st
    import random
except ModuleNotFoundError:
    print("Le module 'streamlit' n'est pas installÃ©. Veuillez l'installer avec 'pip install streamlit'.")
    exit()

# Fonctions utilitaires pour l'apprentissage par renforcement
def calculate_reward(position, environment):
    """Calcule la rÃ©compense basÃ©e sur la position actuelle"""
    rewards = {
        "ğŸ•³ï¸": -10,  # PiÃ¨ge
        "ğŸ’°": 15,   # TrÃ©sor
        "ğŸ¤–": 0,    # Position neutre
        "ğŸ": -5,   # Serpent
        "ğŸ’": 25    # Diamant
    }
    return rewards.get(environment[position], 0)

def show_reward_feedback(reward):
    """Affiche le feedback basÃ© sur la rÃ©compense"""
    if reward > 20:
        st.success(f"ğŸ‰ FANTASTIQUE ! Vous avez trouvÃ© un diamant ! (+{reward} pts)")
        st.balloons()
    elif reward > 10:
        st.success(f"ğŸ’° Excellent ! TrÃ©sor dÃ©couvert ! (+{reward} pts)")
    elif reward > 0:
        st.info(f"âœ¨ Pas mal ! (+{reward} pts)")
    elif reward == 0:
        st.warning("ğŸ˜ Zone neutre (0 pt)")
    elif reward > -10:
        st.warning(f"ğŸ AÃ¯e ! Vous avez rencontrÃ© un serpent ! ({reward} pts)")
    else:
        st.error(f"ğŸ•³ï¸ OUPS ! Vous Ãªtes tombÃ© dans un piÃ¨ge ! ({reward} pts)")

def show_detailed_feedback_supervised(correct, user_answer, correct_answer, score_gained):
    """Feedback didactique dÃ©taillÃ© pour l'apprentissage supervisÃ©"""
    if correct:
        st.success(f"ğŸ‰ **CORRECT !** Vous avez identifiÃ© un {correct_answer}")
        st.info(f"""
        **ğŸ§  Analyse didactique :**
        - **Processus mental** : Vous avez comparÃ© les caractÃ©ristiques visuelles de l'emoji avec vos connaissances
        - **En IA** : Un modÃ¨le ferait de mÃªme en analysant les pixels, formes et patterns
        - **Apprentissage** : Chaque bonne rÃ©ponse renforce les connexions neuronales
        - **Points gagnÃ©s** : +{score_gained} (rÃ©compense positive = renforcement)
        """)
        with st.expander("ğŸ”¬ Zoom sur l'algorithme"):
            st.markdown("""
            1. **Extraction de caractÃ©ristiques** : L'emoji a des patterns uniques
            2. **Comparaison** : Votre cerveau compare avec sa base de donnÃ©es
            3. **Classification** : Attribution Ã  la classe la plus probable
            4. **Confiance** : DegrÃ© de certitude dans la prÃ©diction
            """)
    else:
        st.error(f"âŒ **INCORRECT !** C'Ã©tait un {correct_answer}, pas un {user_answer}")
        st.warning(f"""
        **ğŸ” Analyse de l'erreur :**
        - **Confusion possible** : SimilaritÃ©s visuelles entre {user_answer} et {correct_answer}
        - **En IA** : C'est ce qu'on appelle une "fausse classification"
        - **Apprentissage** : L'erreur aide Ã  ajuster les poids du modÃ¨le
        - **PÃ©nalitÃ©** : {score_gained} points (signal d'erreur pour l'apprentissage)
        """)
        with st.expander("ğŸ¯ Comment s'amÃ©liorer ?"):
            st.markdown(f"""
            - **Observez mieux** : Quelles sont les caractÃ©ristiques uniques du {correct_answer} ?
            - **MÃ©morisation** : Associez l'emoji Ã  ses traits distinctifs
            - **En IA** : Plus de donnÃ©es d'entraÃ®nement amÃ©liorent la prÃ©cision
            """)

def show_detailed_feedback_unsupervised(user_groups, data):
    """Feedback didactique dÃ©taillÃ© pour l'apprentissage non-supervisÃ©"""
    st.success("ğŸ§© **Analyse de votre regroupement :**")
    
    # Analyse automatique des groupes optimaux
    ages = data['ages']
    revenus = data['revenus']
    
    optimal_groups = []
    if max(ages) - min(ages) > 30:
        optimal_groups.append("ğŸ‘¶ **Jeunes** (20-35 ans)")
        optimal_groups.append("ğŸ‘¨ **ExpÃ©rimentÃ©s** (35-55 ans)")
        optimal_groups.append("ğŸ‘´ **Seniors** (55+ ans)")
    
    st.info(f"""
    **ğŸ” Votre approche vs Algorithme :**
    
    **Votre regroupement :** {user_groups}
    
    **Groupes optimaux dÃ©tectÃ©s :**
    {chr(10).join([f"- {group}" for group in optimal_groups])}
    
    **ğŸ§  Apprentissage didactique :**
    - **SimilaritÃ©** : Vous cherchez des patterns comme l'Ã¢ge ou le revenu similaires
    - **Distance** : En IA, on calcule la "distance" mathÃ©matique entre les points
    - **CentroÃ¯des** : Chaque groupe a un "centre" reprÃ©sentatif
    - **ItÃ©ration** : L'algorithme ajuste les groupes jusqu'Ã  convergence
    """)
    
    with st.expander("ğŸ”¬ Algorithmes de clustering"):
        st.markdown(f"""
        **K-Means (le plus populaire) :**
        1. **Initialisation** : Choix de {len(optimal_groups)} centres alÃ©atoirement
        2. **Attribution** : Chaque client va au centre le plus proche
        3. **Recalcul** : Les centres se dÃ©placent au milieu de leur groupe
        4. **RÃ©pÃ©tition** : Jusqu'Ã  stabilisation des groupes
        
        **CritÃ¨res de qualitÃ© :**
        - **CohÃ©sion interne** : Clients similaires dans un mÃªme groupe
        - **SÃ©paration externe** : Groupes bien distincts les uns des autres
        """)

def show_detailed_feedback_reinforcement(action, reward, position, episode_score, steps):
    """Feedback didactique dÃ©taillÃ© pour l'apprentissage par renforcement"""
    environment_names = {0: "ğŸ•³ï¸ PiÃ¨ge", 1: "ğŸ’° TrÃ©sor", 2: "ğŸ¤– Neutre", 3: "ğŸ Serpent", 4: "ğŸ’ Diamant"}
    current_zone = environment_names.get(position, "Zone inconnue")
    
    if reward > 20:
        st.success(f"ğŸ‰ **RÃ‰COMPENSE MAXIMALE !** {current_zone}")
        st.info(f"""
        **ğŸ† Excellence stratÃ©gique :**
        - **Action** : {action} vers position {position+1}
        - **RÃ©compense** : +{reward} points (signal trÃ¨s positif)
        - **Apprentissage** : Cette action sera **fortement renforcÃ©e**
        - **StratÃ©gie** : MÃ©morisez cette sÃ©quence gagnante !
        """)
        
    elif reward > 0:
        st.success(f"âœ… **BONNE ACTION !** {current_zone}")
        st.info(f"""
        **ğŸ“ˆ ProgrÃ¨s positif :**
        - **RÃ©compense** : +{reward} points
        - **Signal** : Votre choix Ã©tait judicieux
        - **Renforcement** : Cette action a plus de chances d'Ãªtre rÃ©pÃ©tÃ©e
        """)
        
    elif reward == 0:
        st.warning(f"ğŸ˜ **ACTION NEUTRE** {current_zone}")
        st.info("""
        **ğŸ¤” Pas d'apprentissage :**
        - **RÃ©compense** : 0 point (signal neutre)
        - **Effet** : Aucun renforcement positif ou nÃ©gatif
        - **Conseil** : Explorez d'autres actions pour apprendre
        """)
        
    else:
        st.error(f"âŒ **PUNITION !** {current_zone}")
        st.warning(f"""
        **âš ï¸ Signal nÃ©gatif :**
        - **PÃ©nalitÃ©** : {reward} points
        - **Apprentissage** : Cette action sera **dÃ©couragÃ©e**
        - **StratÃ©gie** : Ã‰vitez cette zone Ã  l'avenir
        - **Adaptation** : L'agent apprend de ses erreurs
        """)
    
    # Analyse stratÃ©gique globale
    with st.expander("ğŸ¯ Analyse stratÃ©gique avancÃ©e"):
        performance = episode_score / max(steps, 1)
        st.markdown(f"""
        **ğŸ“Š Performance actuelle :**
        - **Score/Action** : {performance:.1f} points par action
        - **EfficacitÃ©** : {'ğŸŸ¢ Excellente' if performance > 5 else 'ğŸŸ¡ Moyenne' if performance > 0 else 'ğŸ”´ Ã€ amÃ©liorer'}
        
        **ğŸ§  Concepts clÃ©s :**
        - **Exploration vs Exploitation** : Faut-il dÃ©couvrir ou rÃ©pÃ©ter les bonnes actions ?
        - **Fonction de valeur** : Chaque position a une "valeur" espÃ©rÃ©e
        - **Politique optimale** : La meilleure stratÃ©gie Ã  long terme
        - **Discount factor** : Les rÃ©compenses futures valent-elles moins ?
        """)

# Configuration de l'application
st.set_page_config(page_title="AI Learning Game", page_icon="ğŸ¤–", layout="wide")

# En-tÃªte avec style
st.markdown("""
<div style='text-align: center; padding: 2rem; background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); border-radius: 10px; margin-bottom: 2rem;'>
    <h1 style='color: white; font-size: 3rem; margin-bottom: 0.5rem;'>ğŸ¤– AI Learning Game</h1>
    <p style='color: white; font-size: 1.2rem; margin: 0;'>DÃ©couvrez l'apprentissage automatique de maniÃ¨re interactive !</p>
</div>
""", unsafe_allow_html=True)

# Initialisation du score dans la session
if 'score_total' not in st.session_state:
    st.session_state.score_total = 0
if 'questions_repondues' not in st.session_state:
    st.session_state.questions_repondues = 0

# Affichage du score
col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    st.metric("ğŸ† Score Total", st.session_state.score_total, f"Questions: {st.session_state.questions_repondues}")

st.markdown("---")

# SÃ©lection du type d'apprentissage
st.markdown("### ğŸ“š Choisissez votre domaine d'apprentissage :")
type_apprentissage = st.selectbox("", [
    "ğŸ” Apprentissage supervisÃ©",
    "ğŸ§© Apprentissage non-supervisÃ©", 
    "ğŸ® Apprentissage par renforcement"
], format_func=lambda x: x)

if type_apprentissage == "ğŸ” Apprentissage supervisÃ©":
    st.markdown("## ğŸ” Apprentissage SupervisÃ© - Classification d'Images")
    
    # Zone d'information
    with st.expander("â„¹ï¸ Qu'est-ce que l'apprentissage supervisÃ© ?", expanded=False):
        st.markdown("""
        **L'apprentissage supervisÃ©** utilise des donnÃ©es Ã©tiquetÃ©es pour entraÃ®ner un modÃ¨le.
        
        **Exemples concrets :**
        - ğŸ“§ DÃ©tection de spam dans les emails
        - ğŸ¥ Diagnostic mÃ©dical Ã  partir d'images
        - ğŸš— Reconnaissance de panneaux de signalisation
        - ğŸ’¬ Analyse de sentiment dans les commentaires
        """)
    
    st.markdown("### ğŸ¯ DÃ©fi : Identifiez correctement l'animal !")
    
    # GÃ©nÃ©ration d'un nouvel animal Ã  chaque rafraÃ®chissement
    if 'current_animal' not in st.session_state:
        images = {"Chat": "ğŸ±", "Chien": "ğŸ¶", "Oiseau": "ğŸ¦", "Poisson": "ğŸ ", "Lapin": "ğŸ°", "Panda": "ğŸ¼"}
        st.session_state.current_animal = random.choice(list(images.items()))
    
    label_correct, image = st.session_state.current_animal
    
    # Affichage stylÃ© de l'image
    col1, col2, col3 = st.columns([1, 1, 1])
    with col2:
        st.markdown(f"""
        <div style='text-align: center; padding: 2rem; background-color: #f0f2f6; border-radius: 15px; margin: 1rem 0;'>
            <div style='font-size: 5rem; margin-bottom: 1rem;'>{image}</div>
            <p style='font-size: 1.2rem; color: #333;'>Quel animal voyez-vous ?</p>
        </div>
        """, unsafe_allow_html=True)
    
    # Choix de l'utilisateur
    images_all = {"Chat": "ğŸ±", "Chien": "ğŸ¶", "Oiseau": "ğŸ¦", "Poisson": "ğŸ ", "Lapin": "ğŸ°", "Panda": "ğŸ¼"}
    label_utilisateur = st.selectbox("ğŸ¤” Votre rÃ©ponse :", list(images_all.keys()))
    
    col1, col2, col3 = st.columns([1, 1, 1])
    with col2:
        if st.button("âœ… Valider ma rÃ©ponse", use_container_width=True):
            st.session_state.questions_repondues += 1
            if label_utilisateur == label_correct:
                score_gained = 10
                st.session_state.score_total += score_gained
                show_detailed_feedback_supervised(True, label_utilisateur, label_correct, score_gained)
                st.balloons()
            else:
                score_gained = -2
                st.session_state.score_total = max(0, st.session_state.score_total + score_gained)
                show_detailed_feedback_supervised(False, label_utilisateur, label_correct, score_gained)
            
            # GÃ©nÃ©rer un nouvel animal
            st.session_state.current_animal = random.choice(list(images_all.items()))
    
    # Bouton pour une nouvelle question
    if st.button("ğŸ”„ Nouvel animal", use_container_width=True):
        st.session_state.current_animal = random.choice(list(images_all.items()))
        st.rerun()
    
    # Explication dÃ©taillÃ©e
    st.markdown("---")
    st.markdown("### ğŸ“– Comment Ã§a marche ?")
    st.markdown("""
    1. **DonnÃ©es d'entraÃ®nement** : Le modÃ¨le apprend avec des milliers d'images Ã©tiquetÃ©es
    2. **Extraction de caractÃ©ristiques** : Il identifie les patterns (formes, couleurs, textures)
    3. **PrÃ©diction** : Sur une nouvelle image, il prÃ©dit la classe la plus probable
    4. **Ã‰valuation** : On mesure la prÃ©cision sur des donnÃ©es de test
    """)

elif type_apprentissage == "ğŸ§© Apprentissage non-supervisÃ©":
    st.markdown("## ğŸ§© Apprentissage Non-SupervisÃ© - Clustering de DonnÃ©es")
    
    # Zone d'information
    with st.expander("â„¹ï¸ Qu'est-ce que l'apprentissage non-supervisÃ© ?", expanded=False):
        st.markdown("""
        **L'apprentissage non-supervisÃ©** dÃ©couvre des structures cachÃ©es dans les donnÃ©es sans Ã©tiquettes.
        
        **Exemples concrets :**
        - ğŸ›’ Segmentation de clients pour le marketing
        - ğŸ§¬ Analyse de sÃ©quences gÃ©nÃ©tiques
        - ğŸ“Š DÃ©tection d'anomalies dans les transactions
        - ğŸµ SystÃ¨mes de recommandation musicale
        """)
    
    st.markdown("### ğŸ¯ DÃ©fi : Regroupez les donnÃ©es par similaritÃ© !")
    
    # GÃ©nÃ©ration de donnÃ©es plus rÃ©alistes
    if 'current_data' not in st.session_state:
        # Simuler des donnÃ©es de clients avec Ã¢ge et revenu
        st.session_state.current_data = {
            'ages': [22, 25, 35, 38, 45, 48, 52, 55, 62, 65],
            'revenus': [25000, 30000, 45000, 50000, 65000, 70000, 75000, 80000, 85000, 90000]
        }
    
    data = st.session_state.current_data
    
    # Visualisation des donnÃ©es
    st.markdown("**ğŸ“Š DonnÃ©es clients (Ã‚ge vs Revenu) :**")
    
    # Affichage en tableau
    import pandas as pd
    df = pd.DataFrame({
        'Client': [f'Client {i+1}' for i in range(len(data['ages']))],
        'Ã‚ge': data['ages'],
        'Revenu (â‚¬)': data['revenus']
    })
    st.dataframe(df, use_container_width=True)
    
    # Simulation d'un graphique simple avec du texte
    st.markdown("""
    ```
    Revenu (â‚¬)
    90000 |                    â€¢  â€¢  â€¢
    80000 |               â€¢  â€¢
    70000 |          â€¢  â€¢
    60000 |     â€¢  â€¢
    50000 |â€¢  â€¢
    40000 |
    30000 |
    20000 +----+----+----+----+----+----+
          20   30   40   50   60   70  Ã‚ge
    ```
    """)
    
    st.markdown("### ğŸ¤” Votre analyse :")
    groupes = st.text_area(
        "Comment regrouperiez-vous ces clients ? (ex: Jeunes/Seniors, Revenus faibles/Ã©levÃ©s)",
        placeholder="Exemple: Groupe 1: Clients 1-4 (jeunes, revenus modestes)\nGroupe 2: Clients 5-10 (seniors, revenus Ã©levÃ©s)"
    )
    
    if st.button("ğŸ” Analyser mon regroupement"):
        if groupes:
            show_detailed_feedback_unsupervised(groupes, data)
        else:
            st.warning("Veuillez proposer un regroupement !")
    
    if st.button("ğŸ”„ Nouvelles donnÃ©es", use_container_width=True):
        # GÃ©nÃ©rer de nouvelles donnÃ©es
        ages = sorted([random.randint(20, 70) for _ in range(10)])
        revenus = sorted([random.randint(20000, 100000) for _ in range(10)])
        st.session_state.current_data = {'ages': ages, 'revenus': revenus}
        st.rerun()
    
    # Explication dÃ©taillÃ©e
    st.markdown("---")
    st.markdown("### ğŸ“– Algorithmes de clustering populaires :")
    st.markdown("""
    - **K-means** : Divise en K groupes selon la distance euclidienne
    - **Clustering hiÃ©rarchique** : CrÃ©e un arbre de regroupements
    - **DBSCAN** : DÃ©tecte les groupes de densitÃ© variable
    - **Gaussian Mixture** : ModÃ©lise les groupes comme des distributions gaussiennes
    """)

elif type_apprentissage == "ğŸ® Apprentissage par renforcement":
    st.markdown("## ğŸ® Apprentissage par Renforcement - Agent Explorateur")
    
    # Zone d'information
    with st.expander("â„¹ï¸ Qu'est-ce que l'apprentissage par renforcement ?", expanded=False):
        st.markdown("""
        **L'apprentissage par renforcement** apprend par interaction avec l'environnement via rÃ©compenses/punitions.
        
        **Exemples concrets :**
        - ğŸ® IA de jeux vidÃ©o (AlphaGo, OpenAI Five)
        - ğŸš— Voitures autonomes
        - ğŸ’° Trading algorithmique
        - ğŸ¤– Robotique et navigation
        """)
    
    # Initialisation de l'environnement
    if 'agent_position' not in st.session_state:
        st.session_state.agent_position = 2  # Position centrale
        st.session_state.episode_score = 0
        st.session_state.steps_taken = 0
        st.session_state.treasure_found = 0
    
    st.markdown("### ğŸ—ºï¸ Environnement : Chasse au TrÃ©sor")
    
    # RÃ¨gles du jeu dÃ©taillÃ©es
    with st.expander("ğŸ“‹ RÃˆGLES DU JEU - Ã€ LIRE AVANT DE COMMENCER", expanded=True):
        st.markdown("""
        ### ğŸ¯ **Objectif :**
        Maximisez votre score en naviguant intelligemment dans l'environnement !
        
        ### ğŸ—ºï¸ **L'environnement :**
        - Vous Ã©voluez sur une **ligne de 5 cases** (positions 1 Ã  5)
        - Chaque case contient un **Ã©lÃ©ment diffÃ©rent** avec sa rÃ©compense
        - Votre position actuelle est marquÃ©e par le robot ğŸ¤–
        
        ### ğŸ® **Actions possibles :**
        - **â¬…ï¸ Aller Ã  Gauche** : Se dÃ©place d'une case vers la gauche (si possible)
        - **â¡ï¸ Aller Ã  Droite** : Se dÃ©place d'une case vers la droite (si possible)  
        - **ğŸ›‘ Rester sur Place** : Ne bouge pas (pÃ©nalitÃ© de -1 point)
        
        ### ğŸ† **SystÃ¨me de rÃ©compenses :**
        - ğŸ•³ï¸ **PiÃ¨ge** : **-10 points** (Attention, trÃ¨s pÃ©nalisant !)
        - ğŸ **Serpent** : **-5 points** (Ã‰vitez-le si possible)
        - ğŸ¤– **Zone neutre** : **0 point** (Pas de rÃ©compense)
        - ğŸ’° **TrÃ©sor** : **+15 points** (Bonne trouvaille !)
        - ğŸ’ **Diamant** : **+25 points** (Jackpot ! La meilleure rÃ©compense)
        
        ### ğŸ§  **StratÃ©gie :**
        - **Explorez** d'abord pour dÃ©couvrir l'environnement
        - **Retenez** oÃ¹ sont les bonnes et mauvaises cases
        - **Optimisez** vos dÃ©placements pour maximiser les gains
        - **Attention** : rester immobile coÃ»te des points !
        """)
    
    # Affichage de l'environnement
    environment = ["ğŸ•³ï¸", "ğŸ’°", "ğŸ¤–", "ğŸ", "ğŸ’"]  # PiÃ¨ge, TrÃ©sor, Agent, Serpent, Diamant
    env_display = environment.copy()
    env_display[st.session_state.agent_position] = "ğŸ¤–"
    
    st.markdown("### ğŸ® **Votre environnement actuel :**")
    
    # Affichage des numÃ©ros de positions
    cols = st.columns(5)
    for i, col in enumerate(cols):
        with col:
            st.markdown(f"<div style='text-align: center; font-weight: bold; color: #666;'>Position {i+1}</div>", unsafe_allow_html=True)
    
    # Affichage de l'environnement avec votre position
    cols = st.columns(5)
    for i, (col, item) in enumerate(zip(cols, env_display)):
        with col:
            if i == st.session_state.agent_position:
                st.markdown(f"<div style='text-align: center; background-color: #90EE90; padding: 1.5rem; border-radius: 15px; font-size: 3rem; border: 3px solid #4CAF50;'>{item}</div>", unsafe_allow_html=True)
                st.markdown("<p style='text-align: center; color: #4CAF50; font-weight: bold; margin-top: 0.5rem;'>VOUS ÃŠTES ICI</p>", unsafe_allow_html=True)
            else:
                st.markdown(f"<div style='text-align: center; padding: 1.5rem; border-radius: 15px; font-size: 3rem; background-color: #f0f2f6;'>{environment[i]}</div>", unsafe_allow_html=True)
    
    # LÃ©gende avec explications
    st.markdown("### ğŸ” **LÃ©gende des Ã©lÃ©ments :**")
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("""
        - ğŸ•³ï¸ **PiÃ¨ge** : -10 points
        - ï¿½ **Serpent** : -5 points  
        - ğŸ¤– **Vous** : Position actuelle
        """)
    with col2:
        st.markdown("""
        - ğŸ’° **TrÃ©sor** : +15 points
        - ğŸ’ **Diamant** : +25 points
        """)
    
    st.markdown("---")
    
    # Statistiques
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ğŸ† Score Ã‰pisode", st.session_state.episode_score)
    with col2:
        st.metric("ğŸ‘£ Pas", st.session_state.steps_taken)
    with col3:
        st.metric("ğŸ’° TrÃ©sors", st.session_state.treasure_found)
    with col4:
        st.metric("ğŸ“ Position", st.session_state.agent_position + 1)
    
    # Actions possibles
    st.markdown("### ğŸ¯ **Choisissez votre prochaine action :**")
    
    # Indication de l'Ã©tat actuel
    current_pos = st.session_state.agent_position + 1
    st.info(f"ğŸ¤– Vous Ãªtes actuellement en **Position {current_pos}** sur l'Ã©lÃ©ment **{environment[st.session_state.agent_position]}**")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        can_go_left = st.session_state.agent_position > 0
        next_left_pos = st.session_state.agent_position - 1 if can_go_left else None
        next_left_element = environment[next_left_pos] if next_left_pos is not None else None
        
        if can_go_left:
            st.markdown(f"**Destination :** Position {next_left_pos + 1} ({next_left_element})")
        else:
            st.markdown("**Impossible** : Vous Ãªtes au bord gauche")
            
        if st.button("â¬…ï¸ Aller Ã  Gauche", disabled=not can_go_left, use_container_width=True):
            st.session_state.agent_position = max(0, st.session_state.agent_position - 1)
            reward = calculate_reward(st.session_state.agent_position, environment)
            st.session_state.episode_score += reward
            st.session_state.steps_taken += 1
            if reward > 0:
                st.session_state.treasure_found += 1
            show_detailed_feedback_reinforcement("Gauche", reward, st.session_state.agent_position, 
                                                st.session_state.episode_score, st.session_state.steps_taken)
            st.rerun()
    
    with col2:
        st.markdown("**Action :** Rester immobile")
        st.markdown("**ConsÃ©quence :** -1 point (pÃ©nalitÃ©)")
        
        if st.button("ğŸ›‘ Rester sur Place", use_container_width=True):
            # Petite pÃ©nalitÃ© pour l'inaction
            st.session_state.episode_score -= 1
            st.session_state.steps_taken += 1
            show_detailed_feedback_reinforcement("Rester", -1, st.session_state.agent_position, 
                                                st.session_state.episode_score, st.session_state.steps_taken)
            st.rerun()
    
    with col3:
        can_go_right = st.session_state.agent_position < 4
        next_right_pos = st.session_state.agent_position + 1 if can_go_right else None
        next_right_element = environment[next_right_pos] if next_right_pos is not None else None
        
        if can_go_right:
            st.markdown(f"**Destination :** Position {next_right_pos + 1} ({next_right_element})")
        else:
            st.markdown("**Impossible** : Vous Ãªtes au bord droit")
            
        if st.button("â¡ï¸ Aller Ã  Droite", disabled=not can_go_right, use_container_width=True):
            st.session_state.agent_position = min(4, st.session_state.agent_position + 1)
            reward = calculate_reward(st.session_state.agent_position, environment)
            st.session_state.episode_score += reward
            st.session_state.steps_taken += 1
            if reward > 0:
                st.session_state.treasure_found += 1
            show_detailed_feedback_reinforcement("Droite", reward, st.session_state.agent_position, 
                                                st.session_state.episode_score, st.session_state.steps_taken)
            st.rerun()
    
    st.markdown("---")
    
    # Conseils stratÃ©giques
    st.markdown("### ğŸ’¡ **Conseils pour une stratÃ©gie optimale :**")
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("""
        **ğŸ¯ Phase d'exploration :**
        - Testez toutes les positions au moins une fois
        - Notez mentalement les rÃ©compenses de chaque case
        - N'ayez pas peur des pÃ©nalitÃ©s au dÃ©but !
        """)
    with col2:
        st.markdown("""
        **ğŸ† Phase d'exploitation :**
        - PrivilÃ©giez les cases avec des rÃ©compenses positives
        - Ã‰vitez les piÃ¨ges et serpents quand vous les connaissez
        - Minimisez les dÃ©placements inutiles
        """)
    
    # Bouton reset avec explication
    col1, col2, col3 = st.columns([1, 1, 1])
    with col2:
        if st.button("ğŸ”„ Nouvel Ã‰pisode", use_container_width=True):
            st.session_state.agent_position = 2
            st.session_state.episode_score = 0
            st.session_state.steps_taken = 0
            st.session_state.treasure_found = 0
            st.success("ğŸ¯ Nouvel Ã©pisode commencÃ© ! Vous repartez de la position centrale.")
            st.rerun()
    
    # Explication dÃ©taillÃ©e
    st.markdown("---")
    st.markdown("### ğŸ“– Concepts clÃ©s :")
    st.markdown("""
    - **Agent** : L'entitÃ© qui prend des dÃ©cisions (vous)
    - **Environnement** : Le monde dans lequel l'agent Ã©volue
    - **Actions** : Les choix possibles (gauche, droite, rester)
    - **RÃ©compenses** : Les signaux de feedback (+/- points)
    - **Politique** : La stratÃ©gie apprise pour maximiser les rÃ©compenses
    """)

# Section Glossaire
st.markdown("---")
st.markdown("## ğŸ“š Glossaire des Termes d'IA")

with st.expander("ğŸ” Apprentissage SupervisÃ© - Termes ClÃ©s"):
    st.markdown("""
    **ğŸ¯ Classification** : PrÃ©dire Ã  quelle catÃ©gorie appartient un Ã©lÃ©ment
    
    **ğŸ·ï¸ Ã‰tiquettes (Labels)** : Les bonnes rÃ©ponses utilisÃ©es pour entraÃ®ner le modÃ¨le
    
    **ğŸ“Š DonnÃ©es d'entraÃ®nement** : Exemples avec leurs rÃ©ponses correctes pour apprendre
    
    **ğŸ§  ModÃ¨le** : L'algorithme qui apprend Ã  faire des prÃ©dictions
    
    **âš–ï¸ Fonction de perte** : Mesure l'erreur entre prÃ©diction et rÃ©alitÃ©
    
    **ğŸ¯ PrÃ©cision** : Pourcentage de bonnes prÃ©dictions sur le total
    
    **ğŸ”„ Surapprentissage** : Quand le modÃ¨le mÃ©morise au lieu de comprendre
    
    **âœ… Validation** : Test sur de nouvelles donnÃ©es pour vÃ©rifier les performances
    
    **ğŸ² Validation croisÃ©e** : Technique pour tester la robustesse du modÃ¨le
    """)

with st.expander("ğŸ§© Apprentissage Non-SupervisÃ© - Termes ClÃ©s"):
    st.markdown("""
    **ğŸ—‚ï¸ Clustering** : Regrouper automatiquement des donnÃ©es similaires
    
    **ğŸ“ CentroÃ¯de** : Point central reprÃ©sentatif d'un groupe
    
    **ğŸ“ Distance euclidienne** : Mesure mathÃ©matique de proximitÃ© entre deux points
    
    **ğŸ¯ K-Means** : Algorithme populaire qui divise en K groupes
    
    **ğŸ”„ Convergence** : Quand l'algorithme trouve une solution stable
    
    **ğŸ“Š Inertie** : Mesure de la compacitÃ© des groupes formÃ©s
    
    **ğŸŒ³ Clustering hiÃ©rarchique** : CrÃ©e un arbre de regroupements
    
    **ğŸ“ˆ DBSCAN** : Trouve des groupes de densitÃ© variable
    
    **ğŸ” Analyse exploratoire** : DÃ©couvrir des patterns cachÃ©s dans les donnÃ©es
    
    **ğŸ“‹ Segmentation** : Division des donnÃ©es en sous-groupes homogÃ¨nes
    """)

with st.expander("ğŸ® Apprentissage par Renforcement - Termes ClÃ©s"):
    st.markdown("""
    **ğŸ¤– Agent** : L'entitÃ© qui prend des dÃ©cisions et apprend
    
    **ğŸŒ Environnement** : Le monde dans lequel l'agent Ã©volue
    
    **âš¡ Action** : Ce que l'agent peut dÃ©cider de faire
    
    **ğŸ RÃ©compense** : Signal positif ou nÃ©gatif reÃ§u aprÃ¨s une action
    
    **ğŸ“‹ Ã‰tat** : Situation actuelle de l'agent dans l'environnement
    
    **ğŸ¯ Politique** : StratÃ©gie que suit l'agent pour choisir ses actions
    
    **ğŸ’° Fonction de valeur** : Estimation de la "valeur" d'un Ã©tat ou action
    
    **ğŸ” Exploration** : Essayer de nouvelles actions pour apprendre
    
    **âš¡ Exploitation** : Utiliser les meilleures actions connues
    
    **ğŸ“‰ Discount factor** : Importance accordÃ©e aux rÃ©compenses futures
    
    **ğŸ² Epsilon-greedy** : StratÃ©gie mixant exploration et exploitation
    
    **ğŸ”„ Q-Learning** : Algorithme populaire d'apprentissage par renforcement
    """)

with st.expander("ğŸ§  Intelligence Artificielle - Concepts GÃ©nÃ©raux"):
    st.markdown("""
    **ğŸ¤– Intelligence Artificielle (IA)** : CapacitÃ© d'une machine Ã  imiter l'intelligence humaine
    
    **ğŸ“– Machine Learning (ML)** : Sous-domaine de l'IA qui apprend Ã  partir de donnÃ©es
    
    **ğŸ§  Deep Learning** : ML utilisant des rÃ©seaux de neurones profonds
    
    **ğŸŒ RÃ©seau de neurones** : ModÃ¨le inspirÃ© du cerveau humain
    
    **âš–ï¸ Algorithme** : Suite d'instructions pour rÃ©soudre un problÃ¨me
    
    **ğŸ“Š Big Data** : TrÃ¨s grandes quantitÃ©s de donnÃ©es difficiles Ã  traiter
    
    **ğŸ”® PrÃ©diction** : Estimation d'un rÃ©sultat futur basÃ©e sur des donnÃ©es
    
    **ğŸ¯ Optimisation** : Recherche de la meilleure solution possible
    
    **ğŸ“ˆ Gradient descent** : MÃ©thode pour minimiser les erreurs du modÃ¨le
    
    **ğŸ”„ ItÃ©ration** : RÃ©pÃ©tition d'un processus pour amÃ©liorer les rÃ©sultats
    
    **ğŸ“‹ Dataset** : Ensemble de donnÃ©es utilisÃ©es pour l'entraÃ®nement
    
    **ğŸ­ Biais** : Tendance systÃ©matique Ã  favoriser certains rÃ©sultats
    
    **ğŸ² Variance** : SensibilitÃ© du modÃ¨le aux variations des donnÃ©es
    """)

st.markdown("---")
st.markdown("*ğŸ“ Plus vous jouez, plus vous maÃ®trisez ces concepts !*")